{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21167d52-1ded-488b-8c9a-8f5f2b51e18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\dell\\anaconda3\\envs\\hypo_gen\\lib\\site-packages (6.0.1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b200df58-09ef-4a38-a3b3-5dd6360b8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"title\": \"Towards the New XAI: A Hypothesis-Driven Approach to Decision Support\\n  Using Evidence\",\n",
      "    \"abstract\": \"Prior research on AI-assisted human decision-making has explored several\\ndifferent explainable AI (XAI) approaches. A recent paper has proposed a\\nparadigm shift calling for hypothesis-driven XAI through a conceptual framework\\ncalled evaluative AI that gives people evidence that supports or refutes\\nhypotheses without necessarily giving a decision-aid recommendation. In this\\npaper, we describe and evaluate an approach for hypothesis-driven XAI based on\\nthe Weight of Evidence (WoE) framework, which generates both positive and\\nnegative evidence for a given hypothesis. Through human behavioural\\nexperiments, we show that our hypothesis-driven approach increases decision\\naccuracy and reduces reliance compared to a recommendation-driven approach and\\nan AI-explanation-only baseline, but with a small increase in under-reliance\\ncompared to the recommendation-driven approach. Further, we show that\\nparticipants used our hypothesis-driven approach in a materially different way\\nto the two baselines.\",\n",
      "    \"link\": \"http://arxiv.org/abs/2402.01292v3\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis\\n  Generation\",\n",
      "    \"abstract\": \"There is growing interest in hypothesis generation with large language models\\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\\nand how can we systematically evaluate methods for hypothesis generation? To\\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\\nLLMs and hypothesis generation methods across multiple aspects, including\\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\\nWe evaluate four state-of-the-art LLMs combined with six existing\\nhypothesis-generation methods. Overall, our results suggest that existing\\nmethods are capable of discovering valid and novel patterns in the data.\\nHowever, the results from synthetic datasets indicate that there is still\\nsignificant room for improvement, as current hypothesis generation methods do\\nnot fully uncover all relevant or meaningful patterns. Specifically, in\\nsynthetic settings, as task difficulty increases, performance significantly\\ndrops, with best models and methods only recovering 38.8% of the ground-truth\\nhypotheses. These findings highlight challenges in hypothesis generation and\\ndemonstrate that HypoBench serves as a valuable resource for improving AI\\nsystems designed to assist scientific discovery.\",\n",
      "    \"link\": \"http://arxiv.org/abs/2504.11524v1\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Human Learning about AI\",\n",
      "    \"abstract\": \"We study how people form expectations about the performance of artificial\\nintelligence (AI) and consequences for AI adoption. Our main hypothesis is that\\npeople rely on human-relevant task features when evaluating AI, treating AI\\nfailures on human-easy tasks, and successes on human-difficult tasks, as highly\\ninformative of its overall performance. In lab experiments, we show that\\nprojection of human difficulty onto AI predictably distorts subjects' beliefs\\nand can lead to suboptimal adoption, as failing human-easy tasks need not imply\\npoor overall performance for AI. We find evidence for projection in a field\\nexperiment with an AI giving parenting advice. Potential users strongly infer\\nfrom answers that are equally uninformative but less humanly-similar to\\nexpected answers, significantly reducing trust and future engagement. Our\\nresults suggest AI \\\"anthropomorphism\\\" can backfire by increasing projection and\\nde-aligning people's expectations and AI performance.\",\n",
      "    \"link\": \"http://arxiv.org/abs/2406.05408v2\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# 01_paper_ingestion.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Function to fetch papers from arXiv\n",
    "def fetch_arxiv_papers(query, max_results=5):\n",
    "    \"\"\"\n",
    "    Fetches papers from arXiv based on a search query.\n",
    "    Args:\n",
    "        query (str): Search keyword(s)\n",
    "        max_results (int): Number of papers to fetch\n",
    "    Returns:\n",
    "        List of dicts containing title, abstract, and link\n",
    "    \"\"\"\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse XML response using lxml\n",
    "    soup = BeautifulSoup(response.text, \"lxml-xml\")\n",
    "    \n",
    "    entries = soup.find_all(\"entry\")\n",
    "    papers = []\n",
    "    \n",
    "    for entry in entries:\n",
    "        title = entry.title.text.strip()\n",
    "        abstract = entry.summary.text.strip()\n",
    "        link = entry.id.text.strip()\n",
    "        papers.append({\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"link\": link\n",
    "        })\n",
    "        \n",
    "    return papers\n",
    "\n",
    "# Fetch papers\n",
    "papers = fetch_arxiv_papers(\"AI hypothesis generation\", max_results=3)\n",
    "\n",
    "# Print results in JSON format\n",
    "print(json.dumps(papers, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d1c3362-6ca6-4fec-808d-129b458f40a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved papers to data/arxiv_papers.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Save papers\n",
    "with open(\"data/arxiv_papers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved papers to data/arxiv_papers.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "950479f0-65ea-49bd-8397-f96846b11198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed abstracts for NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "for paper in papers:\n",
    "    abstract = paper[\"abstract\"]\n",
    "    clean_abstract = \" \".join(abstract.split())  # remove newlines and extra spaces\n",
    "    paper[\"clean_abstract\"] = clean_abstract\n",
    "\n",
    "print(\"Preprocessed abstracts for NLP tasks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4515693f-1cf5-40a1-b3c0-3fe1e6ca21be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0698921f-a4ba-47cd-9a1e-e6fd2e5e110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, re\n",
    "\n",
    "# Ensure folder exists\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# Load papers either from memory or from saved file\n",
    "try:\n",
    "    papers  # check if variable exists\n",
    "except NameError:\n",
    "    with open(\"data/arxiv_papers.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        papers = json.load(f)\n",
    "\n",
    "# Write each paper to its own JSON in data/raw/\n",
    "def safe_name(text, maxlen=60):\n",
    "    t = re.sub(r\"[^\\w\\-]+\", \"_\", text)\n",
    "    return t[:maxlen].strip(\"_\") or \"paper\"\n",
    "\n",
    "for i, p in enumerate(papers, start=1):\n",
    "    rec = {\"title\": p.get(\"title\",\"\"), \"abstract\": p.get(\"abstract\",\"\")}\n",
    "    fname = f\"{i:02d}_\" + safe_name(rec[\"title\"]) + \".json\"\n",
    "    with open(os.path.join(\"data\", \"raw\", fname), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(rec, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "len(os.listdir(\"data/raw\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1e2ee32-ad56-424d-ac9e-4943b92084ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete! Saved to data/processed/abstracts.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "# Define paths\n",
    "RAW_DIR = \"data/raw\"\n",
    "PROCESSED_FILE = \"data/processed/abstracts.json\"\n",
    "\n",
    "# Preprocessing function\n",
    "def clean_text(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalize spaces\n",
    "    return text\n",
    "\n",
    "def preprocess_abstracts(raw_dir):\n",
    "    processed = {}\n",
    "    \n",
    "    for fname in os.listdir(raw_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "        \n",
    "        path = os.path.join(raw_dir, fname)\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            paper = json.load(f)\n",
    "        \n",
    "        abstract = paper.get(\"abstract\", \"\")\n",
    "        if not abstract:\n",
    "            continue\n",
    "        \n",
    "        abstract = clean_text(abstract)\n",
    "        sentences = sent_tokenize(abstract)\n",
    "        \n",
    "        processed[fname] = sentences\n",
    "    \n",
    "    return processed\n",
    "\n",
    "# Run preprocessing\n",
    "processed_abstracts = preprocess_abstracts(RAW_DIR)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Save results\n",
    "with open(PROCESSED_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_abstracts, f, indent=2)\n",
    "\n",
    "print(f\"✅ Preprocessing complete! Saved to {PROCESSED_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc994e-e58f-4f7e-a21b-3c8785e030eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82213d2-2139-4b6f-bf05-d72cb8df6c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
