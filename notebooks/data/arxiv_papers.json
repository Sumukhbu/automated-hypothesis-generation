[
  {
    "title": "Towards the New XAI: A Hypothesis-Driven Approach to Decision Support\n  Using Evidence",
    "abstract": "Prior research on AI-assisted human decision-making has explored several\ndifferent explainable AI (XAI) approaches. A recent paper has proposed a\nparadigm shift calling for hypothesis-driven XAI through a conceptual framework\ncalled evaluative AI that gives people evidence that supports or refutes\nhypotheses without necessarily giving a decision-aid recommendation. In this\npaper, we describe and evaluate an approach for hypothesis-driven XAI based on\nthe Weight of Evidence (WoE) framework, which generates both positive and\nnegative evidence for a given hypothesis. Through human behavioural\nexperiments, we show that our hypothesis-driven approach increases decision\naccuracy and reduces reliance compared to a recommendation-driven approach and\nan AI-explanation-only baseline, but with a small increase in under-reliance\ncompared to the recommendation-driven approach. Further, we show that\nparticipants used our hypothesis-driven approach in a materially different way\nto the two baselines.",
    "link": "http://arxiv.org/abs/2402.01292v3"
  },
  {
    "title": "HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis\n  Generation",
    "abstract": "There is growing interest in hypothesis generation with large language models\n(LLMs). However, fundamental questions remain: what makes a good hypothesis,\nand how can we systematically evaluate methods for hypothesis generation? To\naddress this, we introduce HypoBench, a novel benchmark designed to evaluate\nLLMs and hypothesis generation methods across multiple aspects, including\npractical utility, generalizability, and hypothesis discovery rate. HypoBench\nincludes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.\nWe evaluate four state-of-the-art LLMs combined with six existing\nhypothesis-generation methods. Overall, our results suggest that existing\nmethods are capable of discovering valid and novel patterns in the data.\nHowever, the results from synthetic datasets indicate that there is still\nsignificant room for improvement, as current hypothesis generation methods do\nnot fully uncover all relevant or meaningful patterns. Specifically, in\nsynthetic settings, as task difficulty increases, performance significantly\ndrops, with best models and methods only recovering 38.8% of the ground-truth\nhypotheses. These findings highlight challenges in hypothesis generation and\ndemonstrate that HypoBench serves as a valuable resource for improving AI\nsystems designed to assist scientific discovery.",
    "link": "http://arxiv.org/abs/2504.11524v1"
  },
  {
    "title": "Human Learning about AI",
    "abstract": "We study how people form expectations about the performance of artificial\nintelligence (AI) and consequences for AI adoption. Our main hypothesis is that\npeople rely on human-relevant task features when evaluating AI, treating AI\nfailures on human-easy tasks, and successes on human-difficult tasks, as highly\ninformative of its overall performance. In lab experiments, we show that\nprojection of human difficulty onto AI predictably distorts subjects' beliefs\nand can lead to suboptimal adoption, as failing human-easy tasks need not imply\npoor overall performance for AI. We find evidence for projection in a field\nexperiment with an AI giving parenting advice. Potential users strongly infer\nfrom answers that are equally uninformative but less humanly-similar to\nexpected answers, significantly reducing trust and future engagement. Our\nresults suggest AI \"anthropomorphism\" can backfire by increasing projection and\nde-aligning people's expectations and AI performance.",
    "link": "http://arxiv.org/abs/2406.05408v2"
  }
]