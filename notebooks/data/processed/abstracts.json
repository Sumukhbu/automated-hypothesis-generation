{
  "01_Towards_the_New_XAI_A_Hypothesis-Driven_Approach_to_Decision.json": [
    "Prior research on AI-assisted human decision-making has explored several different explainable AI (XAI) approaches.",
    "A recent paper has proposed a paradigm shift calling for hypothesis-driven XAI through a conceptual framework called evaluative AI that gives people evidence that supports or refutes hypotheses without necessarily giving a decision-aid recommendation.",
    "In this paper, we describe and evaluate an approach for hypothesis-driven XAI based on the Weight of Evidence (WoE) framework, which generates both positive and negative evidence for a given hypothesis.",
    "Through human behavioural experiments, we show that our hypothesis-driven approach increases decision accuracy and reduces reliance compared to a recommendation-driven approach and an AI-explanation-only baseline, but with a small increase in under-reliance compared to the recommendation-driven approach.",
    "Further, we show that participants used our hypothesis-driven approach in a materially different way to the two baselines."
  ],
  "02_HypoBench_Towards_Systematic_and_Principled_Benchmarking_for.json": [
    "There is growing interest in hypothesis generation with large language models (LLMs).",
    "However, fundamental questions remain: what makes a good hypothesis, and how can we systematically evaluate methods for hypothesis generation?",
    "To address this, we introduce HypoBench, a novel benchmark designed to evaluate LLMs and hypothesis generation methods across multiple aspects, including practical utility, generalizability, and hypothesis discovery rate.",
    "HypoBench includes 7 real-world tasks and 5 synthetic tasks with 194 distinct datasets.",
    "We evaluate four state-of-the-art LLMs combined with six existing hypothesis-generation methods.",
    "Overall, our results suggest that existing methods are capable of discovering valid and novel patterns in the data.",
    "However, the results from synthetic datasets indicate that there is still significant room for improvement, as current hypothesis generation methods do not fully uncover all relevant or meaningful patterns.",
    "Specifically, in synthetic settings, as task difficulty increases, performance significantly drops, with best models and methods only recovering 38.8% of the ground-truth hypotheses.",
    "These findings highlight challenges in hypothesis generation and demonstrate that HypoBench serves as a valuable resource for improving AI systems designed to assist scientific discovery."
  ],
  "03_Human_Learning_about_AI.json": [
    "We study how people form expectations about the performance of artificial intelligence (AI) and consequences for AI adoption.",
    "Our main hypothesis is that people rely on human-relevant task features when evaluating AI, treating AI failures on human-easy tasks, and successes on human-difficult tasks, as highly informative of its overall performance.",
    "In lab experiments, we show that projection of human difficulty onto AI predictably distorts subjects' beliefs and can lead to suboptimal adoption, as failing human-easy tasks need not imply poor overall performance for AI.",
    "We find evidence for projection in a field experiment with an AI giving parenting advice.",
    "Potential users strongly infer from answers that are equally uninformative but less humanly-similar to expected answers, significantly reducing trust and future engagement.",
    "Our results suggest AI \"anthropomorphism\" can backfire by increasing projection and de-aligning people's expectations and AI performance."
  ]
}