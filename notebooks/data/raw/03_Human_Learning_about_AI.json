{
  "title": "Human Learning about AI",
  "abstract": "We study how people form expectations about the performance of artificial\nintelligence (AI) and consequences for AI adoption. Our main hypothesis is that\npeople rely on human-relevant task features when evaluating AI, treating AI\nfailures on human-easy tasks, and successes on human-difficult tasks, as highly\ninformative of its overall performance. In lab experiments, we show that\nprojection of human difficulty onto AI predictably distorts subjects' beliefs\nand can lead to suboptimal adoption, as failing human-easy tasks need not imply\npoor overall performance for AI. We find evidence for projection in a field\nexperiment with an AI giving parenting advice. Potential users strongly infer\nfrom answers that are equally uninformative but less humanly-similar to\nexpected answers, significantly reducing trust and future engagement. Our\nresults suggest AI \"anthropomorphism\" can backfire by increasing projection and\nde-aligning people's expectations and AI performance."
}