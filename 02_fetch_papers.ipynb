{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce3a0948-8279-4c30-bef6-48a07e4badbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ feedparser is installed!\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "print(\"✅ feedparser is installed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b2b0c6-0b99-472a-8515-889854b6d7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 5 papers into data/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import feedparser\n",
    "import json\n",
    "\n",
    "# Define raw data directory\n",
    "RAW_DIR = \"data/raw\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "def fetch_arxiv(category=\"cs.AI\", max_results=5):\n",
    "    \"\"\"\n",
    "    Fetch papers from arXiv API and save as JSON in data/raw/.\n",
    "    category: arXiv category (e.g., cs.AI, cs.CL, cs.LG, stat.ML)\n",
    "    max_results: number of papers to fetch\n",
    "    \"\"\"\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=cat:{category}&start=0&max_results={max_results}\"\n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    for i, entry in enumerate(feed.entries):\n",
    "        paper = {\n",
    "            \"id\": entry.id,\n",
    "            \"title\": entry.title,\n",
    "            \"abstract\": entry.summary,\n",
    "            \"authors\": [author.name for author in entry.authors],\n",
    "            \"published\": entry.published\n",
    "        }\n",
    "        with open(os.path.join(RAW_DIR, f\"paper_{i+1}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(paper, f, indent=2)\n",
    "\n",
    "    print(f\"✅ Downloaded {len(feed.entries)} papers into {RAW_DIR}\")\n",
    "\n",
    "# Example: Fetch 5 recent AI papers\n",
    "fetch_arxiv(\"cs.AI\", max_results=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c371774-ceeb-4034-84e5-74dee3a9c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Download complete! Saved 10 papers to data/raw\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import feedparser\n",
    "import urllib.parse  # <-- for URL encoding\n",
    "\n",
    "# Define raw data directory\n",
    "RAW_DIR = \"data/raw\"\n",
    "os.makedirs(RAW_DIR, exist_ok=True)\n",
    "\n",
    "# Function to fetch papers from arXiv\n",
    "def fetch_arxiv_papers(query=\"machine learning\", max_results=5):\n",
    "    \"\"\"\n",
    "    Fetch abstracts from arXiv API\n",
    "    query: search term (default \"machine learning\")\n",
    "    max_results: number of papers to fetch\n",
    "    \"\"\"\n",
    "    query = urllib.parse.quote(query)  # <-- encode query safely\n",
    "    url = f\"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    papers = []\n",
    "    for entry in feed.entries:\n",
    "        paper = {\n",
    "            \"id\": entry.id,\n",
    "            \"title\": entry.title.strip().replace(\"\\n\", \" \"),\n",
    "            \"abstract\": entry.summary.strip().replace(\"\\n\", \" \"),\n",
    "            \"authors\": [author.name for author in entry.authors],\n",
    "            \"published\": entry.published\n",
    "        }\n",
    "        papers.append(paper)\n",
    "\n",
    "    return papers\n",
    "\n",
    "# Fetch papers (now works with spaces in query)\n",
    "papers = fetch_arxiv_papers(query=\"artificial intelligence\", max_results=10)\n",
    "\n",
    "# Save each paper as a JSON file inside data/raw\n",
    "for i, paper in enumerate(papers):\n",
    "    fname = os.path.join(RAW_DIR, f\"paper_{i+1}.json\")\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(paper, f, indent=2)\n",
    "\n",
    "print(f\"✅ Download complete! Saved {len(papers)} papers to {RAW_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a0550-c0eb-4d6f-b95d-9b92254c0b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "RAW_DIR = \"data/raw\"\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "all_papers = []\n",
    "\n",
    "# Load all JSON files from data/raw\n",
    "for fname in os.listdir(RAW_DIR):\n",
    "    if fname.endswith(\".json\"):\n",
    "        with open(os.path.join(RAW_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "            paper = json.load(f)\n",
    "            all_papers.append({\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"abstract\": paper[\"abstract\"],\n",
    "                \"authors\": paper[\"authors\"],\n",
    "                \"published\": paper[\"published\"]\n",
    "            })\n",
    "\n",
    "# Save consolidated file\n",
    "out_path = os.path.join(PROCESSED_DIR, \"abstracts.json\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_papers, f, indent=2)\n",
    "\n",
    "print(f\"✅ Processed {len(all_papers)} papers into {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56de1904-cd12-4cf0-a56f-a42d109cc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 10 papers into data/processed\\abstracts.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "RAW_DIR = \"data/raw\"\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "all_papers = []\n",
    "\n",
    "# Load all JSON files from data/raw\n",
    "for fname in os.listdir(RAW_DIR):\n",
    "    if fname.endswith(\".json\"):\n",
    "        with open(os.path.join(RAW_DIR, fname), \"r\", encoding=\"utf-8\") as f:\n",
    "            paper = json.load(f)\n",
    "            all_papers.append({\n",
    "                \"title\": paper[\"title\"],\n",
    "                \"abstract\": paper[\"abstract\"],\n",
    "                \"authors\": paper[\"authors\"],\n",
    "                \"published\": paper[\"published\"]\n",
    "            })\n",
    "\n",
    "# Save consolidated file\n",
    "out_path = os.path.join(PROCESSED_DIR, \"abstracts.json\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_papers, f, indent=2)\n",
    "\n",
    "print(f\"✅ Processed {len(all_papers)} papers into {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46b28ba4-728d-4055-abc8-8407a0a670e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total papers: 10\n",
      "\n",
      "Sample paper:\n",
      "\n",
      "Title: The Governance of Physical Artificial Intelligence\n",
      "Abstract: Physical artificial intelligence can prove to be one of the most important challenges of the artificial intelligence. The governance of physical artificial intelligence would define its responsible intelligent application in the society. ...\n",
      "Authors: Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan\n",
      "Published: 2023-04-06T08:26:38Z\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/processed/abstracts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total papers: {len(data)}\\n\")\n",
    "print(\"Sample paper:\\n\")\n",
    "print(\"Title:\", data[0][\"title\"])\n",
    "print(\"Abstract:\", data[0][\"abstract\"][:500], \"...\")\n",
    "print(\"Authors:\", \", \".join(data[0][\"authors\"]))\n",
    "print(\"Published:\", data[0][\"published\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec54ae7a-5efc-4b05-92b1-6d89a778fcc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
